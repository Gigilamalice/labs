# Introduction and Prerequisites

This hands on lab is a deployment of ACM, and OpenShift on AWS, GCP, and Azure, via Red Hat Advanced Cluster Manager. This lab will have you changing through quite a few interfaces: GCP, AWS, Azure, OCP, your terminal, ACM. We will let you know where you need to be at the start of each section.

This guide is a reference, you will need your own accounts and make substitutions where needed below. 

## General Prerequisites

You will need the following items to be able to complete the lab from beginning to end.

### DNS
In the case of the Solutions Engineering team we have a registered DNS zone `sysdeseng.com` that is hosted by AWS route53. From there, each engineer has a subdomain called `<engineer>.sysdeseng.com` to which everything else should be organized by. To provision clusters on other providers, we have to delegate zones to each public cloud provider: GCP and Azure. For this, we will conver in more detail, but in general you will create a NS record on AWS under the `<engineer>.sysdeseng.com` zone that points to the zone created on GCP or Azure. So, this might look like `<engineer>-gcp.<engineer>.sysdeseng.com` and `<engineer>-azure.<engineer>.sysdeseng.com` and `<engineer>-aws.<engineer>.sysdeseng.com`. This helps us stay organized for validation purposes.

### ACM Access

* Access to the [open-cluster-management](https://github.com/open-cluster-management) github repo
* Access to the [open-cluster-management](https://quay.io/open-cluster-management/multiclusterhub-operator-index) Quay repo
* The ability to deploy an OpenShift cluster with appropriately sized worker nodes

### Cloud Accounts

You will need access to the following accounts with the proper permissions.

* GCP
* AWS
* Azure

# Provision OCP on AWS - Creating an OCP Hub Cluster

In this section you will provision an OpenShift cluster on AWS that will function as the ACM Hub. From there, you will provision all other clusters.

## Prereqs
* AWS account with proper permissions
* DNS configured on AWS
* Pull secret from try.openshift.com
* Get the latest OCP installer and CLI tools.

1. Create an install config. Give it an appropriate name, like `<engineer>-hub.<engineer>.sysdeseng.com`.

```
./openshift-install create install-config --dir=acm-hub
```

2. Modify the worker node size to reflect the following change.

```
compute:
  hyperthreading: Enabled
  platform:
    aws:
      type:
        m4.2xlarge
  replicas: 3
```

3. Provision the cluster.

```
./openshift-install create cluster --dir=acm-hub
```

4. Note the OCP credentials: kubeconfig, kubeadmin user and password

5. Log into the OpenShift cluster via CLI and browse around.

6. Explore the clusters, specifically existing namespaces.

# Provision ACM on OCP cluster - Deploy the Hub

## Prereq
* Access to required private Quay repos under open-cluster-management organization.

## Deploy ACM

**NOTE:** You should be on the hub cluster via OCP CLI

1. Use the following repo to provision ACM. The instructions can be found in the README.md file in the repo.

https://github.com/open-cluster-management/deploy

2. Note the URL for the ACM console.

## Explore

**NOTE:** You should be on the hub cluster via OCP CLI

1. Look at the hive and open-cluster-management namespaces. Check out logs, images, different objects.

```
oc get pods,svc,secrets -n hive
oc get pods,svc,secrets -n open-cluster-management
```

# Provision OCP on AWS - Creating a Spoke
## Prereqs

* AWS Access Key
* AWS Access Secret
* OpenShift Pull Secret
* SSH Public and Private Key

## Create the AWS Cloud Connection

**NOTE:** You should be on the ACM UI here.

1. Log into the ACM console with the OCP kubeadmin user and password. If you don't know the URL, grab it with the following command.

```
oc get route multicloud-console -n open-cluster-management
```

2. Use the top left hamburger menu to expand and browse to `Automate Infrastructure` and then `Clusters`.

3. On the top navigation pane, click `Provider Connections`. 

4. Click `Add Connection`.

5. From the `Provider` dropdown menu, fill out the following.

* Choose `Amazon Web Services`.
* Connection Name `aws-connection`
* Namespace `Default`
* Provide the rest of the information as requested.

6. Click `Create`. You will be redirected to the `Provider Connections` screen.

## Create the Cluster

1. On the top navigation pane, click `Clusters`. 

2. Click `Add Cluster`

3. Click `Create Cluster`

4. Provide a cluster name of `<engineer-aws-spoke-1>`

5. Provide a `Base DNS domain` of `<engineer>.sysdeseng.com`

6. The Red Hat OpenShift distribution should already be checked, select `Amazon Web Services` as the platform to provision to.

7. Select a `Release Image` of either OCP 4.3 or OCP 4.4.

8. Choose the `Provider Connection` that you created earlier AWS.

9. Click `Create`.

10. Immediately move the the `Explore` section below and watch `hive` create the cluster.

## Explore the Environments

In this section you will take a look at both the hub cluster and the deployed cluster to see what changes are made so you know what a successful deployment looks like.

### Explore the hub cluster

**NOTE:** You should be on the hub cluster via OCP CLI

1. Log into the OpenShift hub cluster via CLI.

2. When the cluster creation was initiated, it created a namespace on the hub that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
```

3. Look at the logs for hive to watch the install.

```
oc get pods
oc logs -f <engineer>-aws-new-0-2cl6r-provision-zqjlj -c hive
```

4. Upon successful completion, you'll see a message like this.

```
time="2020-04-09T17:27:35Z" level=info msg="install completed successfully" installID=7zrcr6hh
```

### Explore the AWS Spoke Cluster

**NOTE:** You should be on the ACM UI

When the spoke cluster was deployed, the credentials were retrieved by ACM and can be downloaded through the ACM console.

1. Navigate to the `Clusters` window.

2. Click the name of your cluster.

3. On the top right you can click the `Download configuration` dropdown and obtain the cluster `kubeconfig`.

4. In the middle ofthe screen, in the `Details` section, you can reveal the `kubeadmin` password.

**NOTE:** You should be on the Spoke cluster via OCP CLI

1. Log into the OpenShift hub cluster via CLI.

2. When the spoke cluster creation was initiated, it created a namespace on the hub and spoke that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
oc get all
oc get secrets
```

3. Notice that the namespace is empty.

4. Now change to the `multicluster-endpoint` namespace. This is where the agent is that connects back to the hub.

```
oc project multicluster-endpoint
oc get pods
oc get secrets
oc get all
oc describe route endpoint-workmgr
```


# Provision OCP on Azure - Creating a Spoke

## Prereqs
* OpenShift Pull Secret
* SSH Public and Private Key
* OpenShift Install Binary 
* DNS
* Service Principle JSON output
* Azure CLI client
* Azure account with proper Permissions

## Set up Azure

* Create an Azure account
* Create an initial `Resource Group`
* Configure DNS on Azure

## Configure DNS on Azure

**NOTE:** You should be at the Azure Console

1. In the top search bar in the Azure interface, search for `dns`, then select `DNS Zones` from the results.

2. Click `Add` to create a new Zone.

3. Pick the `Resource Group` that the zone will land in.

4. Provide a `Name` of the zone you want to create. Something like `<engineer>-azure.<engineer>.sysdeseng.com`.

5. Click `Review + Create`.

6. Click `Create`.

7. Once the deployment has succeeded, take a look at the `Deployment Details`, select the zone you just created and copy the value of the `NS record`. It should look like the following.

```
ns1-02.azure-dns.com.
ns2-02.azure-dns.net.
ns3-02.azure-dns.org.
ns4-02.azure-dns.info.
```

8. Move over the the AWS route53 console.

## Configure DNS on AWS route53

**NOTE:** You should be at the AWS Console

1. In AWS, open up the route53 console.

2. Open up the `<engineer>.sysdeseng.com>` zone.

3. On the top navigation menue, click `Create Record Set`.

4. Provide a `Name` of `<engineer>-azure`.

5. Change the `Type`, to `NS`.

6. Copy the NS records from the Azure zone to this NS record.

7. Click `Create`.

## Configure Azure access via CLI

In this section you will configure the Azure CLI and set up a Service Principle.

### Install the Azure CLI

**NOTE:** You should be on your local CLI

1. Install the Azure CLI per your platforms requirements. Fedora requires you to add an Azure repo and install it via `dnf`.

```
sudo dnf install azure-cli
```

2. Configure your Azure CLI environment.

```
az configure
```

### Configure the Service Principle

**NOTE:** You should be on your local CLI

Azure requires a properly configured `Service Principle` to make API calls to it's platform. For this section, you'll need the Azure CLI client and the OpenShift install binary.

1. To create the service principle, follow the existing instructions in the OpenShift Azure installation documentation.

https://docs.openshift.com/container-platform/4.3/installing/installing_azure/installing-azure-account.html#installation-azure-service-principal_installing-azure-account

**NOTE:** _When creating the Service Principle, grab the password that it outputs, you will not see that password again and will have to reset the Service Principle to create a new one._

2. Record the output of the Service Principle creation.

### Create your OpenShift provider JSON file

For this section, you will need the OpenShift installer binary.

1. Take the output of the Service Principle creation to proceed.

2. Create the OpenShift configuration files. Find a directory and execute the following command. When you run the installer, pick `Azure` as the provider, then walk through the rest of the wizard. As a result of this effort, a osServicePrinciple.json file will be created. That file is required input for the Azure Cloud Connection that will be created in the ACM interface.

```
openshift-install create install-config --dir=ocp-azure-config
```

2. Take a look at the `~/.azure/osServicePrincipal.json` file.

3. The rest of the install config files that the installer created can be ignored or discarded.

More information can be found here:

https://docs.openshift.com/container-platform/4.3/installing/installing_azure/installing-azure-default.html#installation-launching-installer_installing-azure-default

## Create the Azure Cloud Connection

**NOTE:** You should be at the ACM console

1. Log into the ACM console with the OCP kubeadmin user and password.

```
oc get route multicloud-console -n open-cluster-management
```

2. Use the top left hamburger menu to expand and browse to `Automate Infrastructure` and then `Clusters`.

3. On the top navigation pane, click `Provider Connections`. 

4. Click `Add Connection`.

5. From the `Provider` dropdown menu, fill out the following.

* Choose `Microsoft Azure`.
* Connection Name `azure-connection`
* Namespace `Default`
* Provide the rest of the information as requested.

6. Click `Create`

## Create the Cluster

1. On the top navigation pane, click `Clusters`.

2. Click `Add Cluster`

3. Click `Create Cluster`

4. Provide a cluster name of `<engineer-azure-spoke-1>`

5. Provide a `Base DNS domain` of `<engineer>-azure.<engineer>.sysdeseng.com`

6. The Red Hat OpenShift distribution should already be checked, select `Microsoft Azure` as the platform to provision to.

7. Select a `Release Image` of either OCP 4.3 or OCP 4.4.

8. Choose the `Provider Connection` that you created earlier for Azure.

9. Click `Create`.

10. Immediately move to the `Explore` section below and watch `hive` create the cluster.

## Explore the Environments

In this section you will take a look at both the hub cluster and the deployed cluster to see what changes are made so you know what a successful deployment looks like.

### Explore the hub cluster

**NOTE:** You should be on your local CLI

1. Log into the OpenShift hub cluster via CLI.

2. When the cluster creation was initiated, it created a namespace on the hub that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
```

3. Look at the logs for hive to watch the install.

```
oc get pods
oc logs -f <engineer>-azure-new-0-2cl6r-provision-zqjlj -c hive
```

### Explore the Azure Spoke Cluster

**NOTE:** You should be on the ACM UI

When the spoke cluster was deployed, the credentials were retrieved by ACM and can be downloaded through the ACM console.

1. Navigate to the `Clusters` window.

2. Click the name of your cluster.

3. On the top right you can click the `Download configuration` dropdown and obtain the cluster `kubeconfig`.

4. In the middle ofthe screen, in the `Details` section, you can reveal the `kubeadmin` password.

**NOTE:** You should be on the Spoke cluster via OCP CLI

1. Log into the OpenShift hub cluster via CLI.

2. When the spoke cluster creation was initiated, it created a namespace on the hub and spoke that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
oc get all
oc get secrets
```

3. Notice that the namespace is empty.

4. Now change to the `multicluster-endpoint` namespace. This is where the agent is that connects back to the hub.

```
oc project multicluster-endpoint
oc get pods
oc get secrets
oc get all
oc describe route endpoint-workmgr
```

# Provision OCP on GCP - Creating a Spoke

## Prereqs
* GCP Service Account JSON
* OpenShift Pull Secret
* It helps to have a dedicated project on GCP to organize resources
* SSH Public and Private Key

## Download the GCP SA JSON 

**NOTE:** You Should be in the GCP console.

We need a way to interact with the GCP API, and that is done via a GCP Service Account. While the interface for GCP may change, in general, you want to go to the `Service Accounts` section of `IAM & Admin` to get the Service Account JSON info. This section presumes that you do not have an existing service account.

1. In GCP navigate to the `Service Accounts` applet via `IAM & Admin` through the main menu.

2. Click `Create Service Account`.

3. Provide the `Service Account Details` of `name`, `Service account ID` and a `Description`, click `Create`.

4. Select a role of `Owner`. Click `Continue`.

5. Click `Create Key`, and select `JSON`, and then `Create`. Save the file to your computer.

More information can be found here:

https://docs.openshift.com/container-platform/4.3/installing/installing_gcp/installing-gcp-account.html#installation-gcp-service-account_installing-gcp-account

## Configure DNS on GCP

1. Navigate to the `Cloud DNS` section of `Network Services` via the main menu.

2. Click `Create Zone` to create a new Zone.

3. Provide a `Zone Name` of the zone you want to create. Something like `<engineer>-gcp`.

4. Provide a DNS name of `<engineer>-gcp.<engineer>.sysdeseng.com`, and then provide a `Description`.

5. Click `Create`.

7. Once the deployment has succeeded, take a look at the `Deployment Details`, select the zone you just created and copy the value of the `NS record`. It should look like the following.

```
ns-cloud-b1.googledomains.com.
ns-cloud-b2.googledomains.com.
ns-cloud-b3.googledomains.com.
ns-cloud-b4.googledomains.com.
```

## Configure DNS on AWS route53

**NOTE:** You should be on the AWS console

1. In AWS, open up the route53 console.

2. Open up the `<engineer>.sysdeseng.com>` zone.

3. On the top navigation menue, click `Create Record Set`.

4. Provide a `Name` of `<engineer>-gcp`.

5. Change the `Type`, to `NS`.

6. Copy the NS records from the gcp zone to this NS record.

7. Click `Create`.

## Create the GCP Cloud Connection

**NOTE:** You should be on the ACM console

1. Log into the ACM console with the OCP kubeadmin user and password.

```
oc get route multicloud-console -n open-cluster-management
```

2. Use the top left hamburger menu to expand and browse to `Automate Infrastructure` and then `Clusters`.

3. On the top navigation pane, click `Provider Connections`. 

4. Click `Add Connection`.

5. From the `Provider` dropdown menu, fill out the following.

* Choose `Google Cloud`.
* Connection Name `gcp-connection`
* Namespace `Default`
* Provide the rest of the information as requested. Including the GCP JSON account key.

6. Click `Create`

## Create the Cluster

1. On the top navigation pane, click `Clusters`.

2. Click `Add Cluster`

3. Click `Create Cluster`

4. Provide a cluster name of `<engineer>-gcp-spoke-1`

5. Provide a `Base DNS domain` of `<engineer>-gcp.<engineer>.sysdeseng.com`

6. The Red Hat OpenShift distribution should already be checked, select `Google Cloud` as the platform to provision to.

7. Select a `Release Image` of either OCP 4.3 or OCP 4.4.

8. Choose the `Provider Connection` that you created earlier for GCP.

9. Click `Create`.

10. Immediately move the the `Explore` section below and watch `hive` create the cluster.

## Explore the Environments

In this section you will take a look at both the hub cluster and the deployed cluster to see what changes are made so you know what a successful deployment looks like.

### Explore the hub cluster

1. Log into the OpenShift hub cluster via CLI.

2. When the cluster creation was initiated, it created a namespace on the hub that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
```

3. Look at the logs for hive to watch the install.

```
oc get pods
oc logs -f <engineer>-gcp-new-0-2cl6r-provision-zqjlj -c hive
```

### Explore the GCP Spoke Cluster

**NOTE:** You should be on the ACM UI

When the spoke cluster was deployed, the credentials were retrieved by ACM and can be downloaded through the ACM console.

1. Navigate to the `Clusters` window.

2. Click the name of your cluster.

3. On the top right you can click the `Download configuration` dropdown and obtain the cluster `kubeconfig`.

4. In the middle of the screen, in the `Details` section, you can reveal the `kubeadmin` password.

**NOTE:** You should be on the Spoke cluster via OCP CLI

1. Log into the OpenShift hub cluster via CLI.

2. When the spoke cluster creation was initiated, it created a namespace on the hub and spoke that matches the name of the cluster that you gave it in the previous steps.  Change to that namespace.

```
oc project <cluster-name>
oc get all
oc get secrets
```

3. Notice that the namespace is empty.

4. Now change to the `multicluster-endpoint` namespace. This is where the agent is that connects back to the hub.

```
oc project multicluster-endpoint
oc get pods
oc get secrets
oc get all
oc describe route endpoint-workmgr
```

# From ACM, deploy an application to each cluster

In this section, you will deploy a simple apache server to each cluster. The web server will use a `ConfigMap` to print out which cluster you are on. In the next lab, the application will be load balanced behind an HAProxy instance to finalize the end to end deployment.

## Prereqs
* OpenShift Clusters Running on Different Providers Managed by ACM
* Github repo Cloned Locally

### Set up your OpenShift Contexts

OC Tool Context Configuration

The following steps will leverage the usage of contexts within the oc tool.

We will be using four contexts:

* `hub` -> Context for accesing HUB Cluster
* `aws-spoke` -> Context for accesing AWS SPOKE Cluster
* `gcp-spoke` -> Context for accesing GCP SPOKE Cluster
* `azure-spoke` -> Context for accesing Azure SPOKE Cluster

Download all of your kubeconfig files for each spoke from ACM. If you don't know how to merge your Kubeconfigs and create the context you can refer to [this documentation[(https://openshift.tips/oc/#merge-multiple-kubeconfigs)

## Provision the Application

1. Explore the spoke clusters. You will notice that there are no application projects in any of the environments.

``` 
oc config use aws-admin
oc config current-context
oc get projects

oc config use gcp-admin
oc config current-context
oc get projects

oc config use azure-admin
oc config current-context
oc get projects
``` 

**NOTE:** To review the application repo, clone it and explore.

```
git clone https://gitlab.cee.redhat.com/scollier/acm-infrastructure.git 
```

2. Deploy the application.

```
cd assets/app/acm
oc --context hub apply -f .
```

3. Explore the spoke clusters and create the route. You will notice that there are now application projects in all of the environments.

``` 
oc --context aws-admin get projects | grep apache
oc --context aws-admin project aws-apache-test
oc --context aws-admin expose service apache-service
oc --context aws-admin get all

oc --context gcp-admin get projects | grep apache
oc --context gcp-admin project gcp-apache-test
oc --context gcp-admin expose service apache-service
oc --context gcp-admin get all

oc --context azure-admin get projects | grep apache
oc --context azure-admin project azure-apache-test
oc --context azure-admin expose service apache-service
oc --context azure-admin get all
``` 


4. Confirm Application Access by making a note of the routes for each cluster above and accessing them directly. Also, they will be used for the configuration of HAProxy. Make sure to replace the DNS name below with your routes.

```
curl http://apache-service-aws-apache-test.apps.aws-spoke-1.<engineer>.sysdeseng.com
curl http://apache-service-gcp-apache-test.apps.gcp-spoke-1.<engineer>.sysdeseng.com
curl http://apache-service-azure-apache-test.apps.azure-spoke-1.<engineer>.sysdeseng.com
```

5. Patch the OpenShift route on each cluster to reflect the DNS name of the HAProxy instance so that OpenShift can interpret the header corretly.

**NOTE:** This should be done in the git repository. However, for now, for the sake of time, we are patching the routes directly.

```
oc patch route apache-service -n gcp-apache-test -p '{"spec":{"host": "haproxy.<engineer>.sysdeseng.com"}}'
oc patch route apache-service -n aws-apache-test -p '{"spec":{"host": "haproxy.<engineer>.sysdeseng.com"}}'
oc patch route apache-service -n azure-apache-test -p '{"spec":{"host": "haproxy.<engineer>.sysdeseng.com"}}'
```

# Configure Load Balancer

Here the HAProxy load balancer will be set up to provide a single point of entry to the Apache server running on each OpenShift cluster.

## Prereqs

* VM on AWS with HAProxy installed. The version of HAProxy used in this lab is 1.8.15 2018/12/13.
* Known OpenShift routes for the applications that were deployed in the previous chapter. These will be used to populate the HAProxy backends.
* It is preferable that you have an elastic IP address associated with a known hostname for your HAProxy instance. This is configured in AWS.
** In this case, it's `haproxy.<engineer>.sysdeseng.com`

## Configure HAProxy

1. Download the following [HAProxy file](https://raw.githubusercontent.com/scollier/acm-app/master/haproxy/haproxy.cfg).

2. Change the bottom of the HAProxy configuration file to reflect the correct DNS names of the routes you have on each cluster for the new applications that were deployed.

3. Confirm the HAProxy file is valid.

```
haproxy -c -f /etc/haproxy/haproxy.cfg
```

3. Restart HAProxy. 

**NOTE:** Depending on the firewall configuration, ports may need to be opened.

```
systemctl restart haproxy
systemctl enable haproxy
```


## Test Application Access via HAProxy

1. Ensure the application can be accessed via curl, or a browser.


```
curl http://haproxy.<engineer>.sysdeseng.com/
<center>
<b>
GCP
</b>
</center>

curl http://haproxy.<engineer>.sysdeseng.com/
<center>
<b>
aws
</b>
</center>

curl http://haproxy.<engineer>.sysdeseng.com/
<center>
<b>
azure
</b>
</center>
```

This concludes the lab.

# Review

In this lab, you have accomplished the following activities.

1. Deployed OpenShift on AWS.
2. Deployed ACM on that cluster to function as a hub.
3. Provisioned OCP on Azure, AWS, and GCP.
	3.1 Which included configuring DNS across each provider.
4. Provisioned an application on each OpenShift cluster.
5. Configured HAProxy to load balance each cluster.
6. Confirmed the application was accessible from a client via HAProxy.
